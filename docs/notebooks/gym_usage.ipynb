{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df413f93",
   "metadata": {},
   "source": [
    "# Operation Safe Passage: Gym Environment\n",
    "\n",
    "The `OSPGym` class is a Gymnasium-compatible wrapper around the Mission Controller.  \n",
    "It provides a reinforcement learning (RL) interface where:\n",
    "\n",
    "- **Actions** are encoded in a fixed `MultiDiscrete` vector.\n",
    "- **Observations** are returned as a flat `Box` of floats.\n",
    "- **Rewards** can use the default shaping or a custom function.\n",
    "- Standard Gymnasium API (`reset`, `step`, `render`) is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99bda91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operation_safe_passage.controller.osp_gym import OSPGym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306cc1f",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The environment requires:\n",
    "- `params.json`: agent and scanner parameters, processing times, etc.\n",
    "- `network.json`: map generated by the Map Generator.\n",
    "\n",
    "Arguments:\n",
    "- `param_path`: path to params file.\n",
    "- `network_path`: path to network file.\n",
    "- `output_dir`: where outputs are written.\n",
    "- `max_steps`: episode truncation step limit (default: 2000).\n",
    "- `reward_fn`: optional custom reward function.\n",
    "- `seed`: RNG seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc87f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OSPGym(\n",
    "    param_path=\"config/params_agents.json\",\n",
    "    network_path=\"config/premade_network.json\",\n",
    "    output_dir=\"output\",\n",
    "    max_steps=200\n",
    ")\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(\"Observation shape:\", obs.shape)\n",
    "print(\"Info keys:\", info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713869c",
   "metadata": {},
   "source": [
    "## Action Space\n",
    "\n",
    "The action vector is a `MultiDiscrete` array in fixed order:\n",
    "[ UAV0_move, UAV0_scan, UAV1_move, UAV1_scan, ..., UGV0_move, UGV1_move, ... ]\n",
    "\n",
    "\n",
    "- **Move**:  \n",
    "  0–5 = directions (`E, NE, NW, W, SW, SE`)  \n",
    "  6 = noop (no move)\n",
    "\n",
    "- **Scan (UAV only)**:  \n",
    "  0..S-1 = scanner index for UAV i  \n",
    "  S = no scan\n",
    "\n",
    "- **UGVs**:  \n",
    "  Only have a `move` entry (no scan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff1fed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# Example random action\n",
    "action = env.action_space.sample()\n",
    "print(\"Random action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c0d254",
   "metadata": {},
   "source": [
    "## Observation Space\n",
    "\n",
    "Observations are returned as a flat float vector (`Box`).\n",
    "\n",
    "For each agent, values are concatenated in this order:\n",
    "\n",
    "```python\n",
    "[ time,\n",
    "distance_to_goal,\n",
    "distance_to_goal_weighted (or -1),\n",
    "current.weight,\n",
    "current.uav_estimate (or -1 if not present),\n",
    "current.temperature,\n",
    "current.wind_speed,\n",
    "current.visibility,\n",
    "current.precipitation,\n",
    "neighbor_weight[E,NE,NW,W,SW,SE] (missing -> -1),\n",
    "distances_to_other_agents (padded to len(agents)-1 with -1)\n",
    "]\n",
    "```\n",
    "\n",
    "- **time**: mission time (float).  \n",
    "- **distance_to_goal**: hex distance from agent to mission end.  \n",
    "- **distance_to_goal_weighted**: terrain-weighted distance (or -1 if not applicable).  \n",
    "- **weight**: node risk weight (e.g., probability of mine).  \n",
    "- **uav_estimate**: UAV scan probability estimate (or -1 if UGV).  \n",
    "- **environment values**: temperature, wind speed, visibility, precipitation.  \n",
    "- **neighbors**: terrain weights of adjacent nodes, ordered by directions.  \n",
    "- **agent distances**: distances to all other agents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaad7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(\"Reward:\", reward)\n",
    "print(\"Terminated:\", terminated)\n",
    "print(\"Truncated:\", truncated)\n",
    "print(\"Obs length:\", len(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475d0cb",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "Default shaping:\n",
    "\n",
    "- `-1` per step  \n",
    "- `+1000` if any UGV reaches the goal  \n",
    "- `-10` if episode truncates without success\n",
    "\n",
    "You can override with a custom function:\n",
    "\n",
    "```python\n",
    "def my_reward(state, terminated, truncated):\n",
    "    return -0.5 + (200 if terminated else 0)\n",
    "\n",
    "env = OSPGym(reward_fn=my_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded74f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "print(\"Episode total reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639cb49d",
   "metadata": {},
   "source": [
    "## Render\n",
    "\n",
    "The `render()` method provides a text summary:\n",
    "\n",
    "- Mission time\n",
    "- Each agent’s distance to goal\n",
    "- Each agent’s current weight\n",
    "\n",
    "Example:\n",
    "```python\n",
    "time=50.0\n",
    "uav_alpha: d_goal=35, w=40.00\n",
    "UGV_0: d_goal=37, w=100.00\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823d25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c04f878",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **OSPGym** wraps the Mission Controller for Gymnasium.\n",
    "- Action space: flat `MultiDiscrete` vector encoding moves/scans.\n",
    "- Observation space: flat `Box` array of concatenated agent features.\n",
    "- Default reward: -1 per step, +1000 on success, -10 on truncation.\n",
    "- Compatible with RL libraries such as `stable-baselines3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c102e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devcom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
